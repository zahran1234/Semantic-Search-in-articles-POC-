{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will compare two models that capture the semantic meaning of text: Word2Vec, a neural network-based model, and Sentence Transformers, which utilizes an attention mechanism for better contextual understanding.\n",
        "\n",
        "Word2Vec: This model, based on a shallow neural network, generates word embeddings that capture semantic relationships between words. It represents each word independently without accounting for surrounding context in a sentence.\n",
        "\n",
        "Sentence Transformers: This advanced model leverages the attention mechanism and transformer architecture, allowing it to capture context across entire sentences, making it highly effective for tasks like semantic search and text similarity.\n",
        "\n",
        "Through this comparison, we will evaluate each modelâ€™s ability to understand and represent semantic meaning advantage of disadvantage of every model , performance time and results performances\n"
      ],
      "metadata": {
        "id": "n4ICQZJ59J6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#requirments packages\n",
        "!pip install chromadb\n",
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xoNDOutFHLa",
        "outputId": "0a8ddda2-a1b4-4ec6-97d7-b5418aa0010b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.5.18)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.9.2)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.6)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.115.4)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.32.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.4)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.20.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.49b1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.6)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.2.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.5)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (31.0.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (5.0.1)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.10)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.27.2)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.9.3)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.2)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.41.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.28.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.65.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.28.1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.49b1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.49b1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b1)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.49b1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b1)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.49b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.49b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.23.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.24.7)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.24.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (13.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.3.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<2.0.36,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.10)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.7)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.15)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.137)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.6->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.6->langchain-community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2.0.36,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community) (2.23.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#needed packages\n",
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import gensim\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score , f1_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from collections import Counter\n",
        "nltk.download('punkt')\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n6lp_Pl2EKC",
        "outputId": "24dd5d91-923b-4395-d697-d51ab9185d75"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **load google word2vec model sementic word repressention The word2vec-google-news-300 model, which is the pre-trained Word2Vec model from Google with 300-dimensional embeddings, was trained using the skip-gram model, not CBOW**"
      ],
      "metadata": {
        "id": "Xjh8nrDU8yVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjI_Sr-U8K7S",
        "outputId": "0afab7f8-7c2b-455b-cf5f-b77901c96db5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Use ChatGPT to generate the test data"
      ],
      "metadata": {
        "id": "7qjYPp28DidA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate sample data for testing: two categories - one for AI-related topics and the other for Sports-related topics.\n",
        "ai_data  = [\n",
        "    # AI and Machine Learning terms\n",
        "    \"algorithm\", \"analytics\", \"automation\", \"augmentation\", \"big data\", \"biometrics\", \"calibration\",\n",
        "    \"classification\", \"clustering\", \"cognitive computing\", \"computer vision\", \"data mining\", \"data processing\",\n",
        "    \"data science\", \"deep learning\", \"detection\", \"feature extraction\", \"genetic algorithms\", \"information retrieval\",\n",
        "    \"Internet of Things (IoT)\", \"knowledge representation\", \"machine learning\", \"natural language processing\",\n",
        "    \"neural networks\", \"optimization\", \"predictive modeling\", \"regression\", \"reinforcement learning\", \"robotics\",\n",
        "    \"sentiment analysis\", \"speech recognition\", \"supervised learning\", \"unsupervised learning\", \"virtual assistants\",\n",
        "    \"virtual reality\", \"visualization\", \"active learning\", \"adversarial networks\", \"artificial neural networks\",\n",
        "    \"backpropagation\", \"batch learning\", \"bias\", \"big data analytics\", \"categorization\", \"chatbots\", \"classification\",\n",
        "    \"clustering\", \"convolutional networks\", \"data augmentation\", \"deep reinforcement learning\", \"ensemble learning\",\n",
        "    \"feature engineering\", \"generative adversarial networks\", \"gradient descent\", \"inference\", \"labeling\",\n",
        "    \"language models\", \"loss function\", \"model evaluation\", \"neural network architecture\", \"overfitting\",\n",
        "    \"parameter tuning\", \"recurrent neural networks\", \"training\", \"transfer learning\", \"underfitting\", \"validation\",\n",
        "    \"vectorization\", \"weights\", \"knowledge graph\", \"deep neural networks\", \"attention mechanism\", \"data pipeline\",\n",
        "    \"sparse coding\", \"neural Turing machines\", \"dynamic time warping\", \"knowledge distillation\", \"autoencoders\",\n",
        "    \"deep autoencoders\", \"unsupervised learning\", \"recommender systems\", \"contextualized embeddings\", \"feature selection\",\n",
        "    \"transformers\", \"BERT\", \"GPT\", \"LSTM\", \"XGBoost\", \"Markov chains\", \"decision trees\", \"support vector machines\",\n",
        "    \"k-means clustering\", \"principal component analysis\", \"feature vectors\", \"text embeddings\", \"self-supervised learning\",\n",
        "    \"boosting\", \"bagging\", \"tuning\", \"dimensionality reduction\", \"cross-validation\", \"regression analysis\", \"data wrangling\",\n",
        "    \"prediction\", \"classification accuracy\", \"performance metrics\", \"bias-variance tradeoff\", \"random forest\", \"ensemble models\",\n",
        "    \"cross entropy\", \"recall\", \"precision\", \"F1-score\", \"ROC curve\", \"AUC\", \"learning rate\", \"convergence\", \"accuracy\" , \"Bayesian inference\", \"data lake\", \"data warehouse\", \"dimensionality reduction\",\n",
        "    \"elastic net\", \"exploratory data analysis\", \"grid search\", \"hyperparameter optimization\",\n",
        "    \"imputation\", \"k-nearest neighbors\", \"linear discriminant analysis\", \"logistic regression\",\n",
        "    \"Markov decision process\", \"multilayer perceptron\", \"natural gradient\", \"nearest centroid\",\n",
        "    \"ordinal regression\", \"probabilistic graphical models\", \"quantile regression\",\n",
        "    \"rejection sampling\", \"sampling methods\", \"semi-supervised learning\", \"signal processing\",\n",
        "    \"spectral clustering\", \"stochastic gradient descent\", \"subsampling\", \"support vector regression\",\n",
        "    \"synthetic data\", \"time series forecasting\", \"tokenization\", \"training set\", \"test set\",\n",
        "    \"validation set\", \"variance inflation factor\", \"weighted loss\", \"word embeddings\", \"zero-shot learning\",\n",
        "    \"Bayesian networks\", \"causal inference\", \"counterfactual analysis\", \"gradient boosting\",\n",
        "    \"hyperplane\", \"latent space\", \"maximum likelihood estimation\", \"model drift\",\n",
        "    \"non-negative matrix factorization\", \"ordinal data\", \"pairwise learning\", \"prototype networks\",\n",
        "    \"quasi-Newton methods\", \"randomized search\", \"representation learning\", \"sample weighting\",\n",
        "    \"smoothing\", \"stacked models\", \"tensor operations\", \"transfer entropy\", \"under-sampling\",\n",
        "    \"variational autoencoders\", \"word2vec\", \"XLNet\", \"BPE (Byte Pair Encoding)\", \"contextual embeddings\",\n",
        "    \"fairness in AI\", \"AI ethics\", \"model interpretability\", \"scalability\", \"online learning\",\n",
        "    \"edge computing\", \"federated learning\", \"privacy-preserving machine learning\", \"self-attention\",\n",
        "    \"multi-task learning\", \"active noise cancellation\", \"ASR (automatic speech recognition)\",\n",
        "    \"chatbots\", \"dialog systems\", \"multi-modal learning\", \"attention heads\", \"memory networks\",\n",
        "    \"sentence transformers\", \"subword tokenization\", \"meta-learning\", \"multi-agent systems\",\n",
        "    \"few-shot learning\", \"multi-layer perceptron\", \"hierarchical clustering\", \"probabilistic programming\",\n",
        "    \"graph neural networks\", \"self-attention mechanism\", \"smart cities\", \"image segmentation\",\n",
        "    \"AI governance\", \"pipeline orchestration\", \"cloud AI services\", \"quantum computing in AI\"\n",
        "\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "# General Sports terms\n",
        "Sports_data=[\"aerobics\", \"agility\", \"athletics\", \"balance\", \"ball\", \"base\", \"basketball\", \"biomechanics\", \"bodybuilding\",\n",
        "    \"boxing\", \"calisthenics\", \"cardio\", \"championship\", \"coach\", \"competition\", \"conditioning\", \"cricket\", \"cycling\",\n",
        "    \"defense\", \"dodgeball\", \"endurance\", \"exercise\", \"fitness\", \"football\", \"goal\", \"gymnastics\", \"half-time\", \"handball\",\n",
        "    \"hockey\", \"interval training\", \"jogging\", \"karate\", \"lacrosse\", \"league\", \"marathon\", \"medal\", \"offense\", \"opponent\",\n",
        "    \"outfield\", \"performance\", \"pitch\", \"player\", \"referee\", \"rugby\", \"running\", \"score\", \"soccer\", \"softball\", \"stamina\",\n",
        "    \"strength\", \"strategy\", \"swimming\", \"team\", \"tennis\", \"tournament\", \"training\", \"triathlon\", \"umpire\", \"uniform\",\n",
        "    \"volleyball\", \"weightlifting\", \"workout\", \"wrestling\", \"kickoff\", \"playoff\", \"punch\", \"race\", \"reflex\", \"ring\",\n",
        "    \"rotation\", \"sprint\", \"teamwork\", \"tackle\", \"workout\", \"wrestler\", \"goalkeeper\", \"injury\", \"tournament\", \"substitute\",\n",
        "    \"huddle\", \"passing\", \"counterattack\", \"dribbling\", \"kick\", \"match\", \"dunk\", \"run\", \"goalkeeper\", \"coach\", \"medal\",\n",
        "    \"ball\", \"bat\", \"paddle\", \"racquet\", \"striker\", \"hurdle\", \"shooting\", \"opponent\", \"defender\", \"goalpost\", \"referee\",\n",
        "    \"playmaker\", \"fastbreak\", \"finals\", \"runner-up\", \"crosscourt\", \"bounce\", \"free throw\", \"assist\", \"rebound\",\n",
        "    \"half-court\", \"corner kick\", \"dribble\", \"offside\", \"tournament\", \"draw\", \"superbowl\", \"relay\", \"marathon\", \"quarterback\",\n",
        "    \"midfield\", \"foul\", \"speedwork\", \"shot put\", \"athlete\", \"long jump\", \"stretches\", \"sprint\", \"outfield\", \"home run\",\n",
        "    \"sacrifice bunt\", \"hit\", \"playbook\", \"refereeing\", \"scoreboard\", \"coach\", \"fitness test\", \"bounce pass\", \"teamwork\",\n",
        "    \"kickoff\", \"team captain\", \"huddle\", \"forward pass\", \"halfback\", \"penalty\", \"game face\", \"double play\", \"goal kick\",\n",
        "    \"offensive\", \"defensive\", \"fast break\", \"cheerleading\", \"time-out\", \"grappling\", \"judo\", \"championship ring\"\n",
        "    \"athletics\", \"baseball\", \"basketball\", \"boxing\", \"climbing\", \"cricket\", \"cycling\",\n",
        "    \"dodgeball\", \"fencing\", \"field hockey\", \"figure skating\", \"football\", \"golf\",\n",
        "    \"gymnastics\", \"handball\", \"hockey\", \"ice skating\", \"karate\", \"kickboxing\", \"lacrosse\",\n",
        "    \"martial arts\", \"mountain biking\", \"netball\", \"parkour\", \"polo\", \"racquetball\",\n",
        "    \"rowing\", \"rugby\", \"sailing\", \"scuba diving\", \"skateboarding\", \"skiing\",\n",
        "    \"snowboarding\", \"soccer\", \"softball\", \"squash\", \"surfing\", \"swimming\", \"table tennis\",\n",
        "    \"taekwondo\", \"tennis\", \"track and field\", \"triathlon\", \"ultimate frisbee\", \"volleyball\",\n",
        "    \"water polo\", \"weightlifting\", \"wrestling\", \"yoga\", \"badminton\", \"cross-country\",\n",
        "    \"freestyle\", \"hurdles\", \"javelin throw\", \"long jump\", \"marathon\", \"pole vault\",\n",
        "    \"relay race\", \"shot put\", \"sprint\", \"archery\", \"biathlon\", \"bowling\", \"canoeing\",\n",
        "    \"equestrian\", \"fishing\", \"kite surfing\", \"motor racing\", \"powerlifting\", \"shooting\",\n",
        "    \"snowmobiling\", \"sprint\", \"trampoline\", \"windsurfing\", \"aerobics\", \"pilates\",\n",
        "    \"mountaineering\", \"free diving\", \"paragliding\", \"bobsleigh\", \"curling\", \"speed skating\",\n",
        "    \"ballet\", \"cheerleading\", \"team sports\", \"individual sports\", \"endurance\", \"strength training\",\n",
        "    \"sports analytics\", \"sports medicine\", \"recovery\", \"fitness\", \"physical training\", \"cardio\",\n",
        "    \"stamina\", \"agility\", \"ball control\", \"jumping ability\", \"aerobic capacity\", \"coordination\",\n",
        "    \"strength\", \"speed\", \"balance\", \"footwork\", \"offense\", \"defense\", \"technique\", \"skills\",\n",
        "    \"competition\", \"teamwork\", \"tactics\", \"strategies\", \"conditioning\", \"refereeing\",\n",
        "    \"sportsmanship\", \"injury prevention\", \"recovery time\", \"personal best\", \"tournament\", \"championship\",\n",
        "    \"league\", \"playoff\", \"match\", \"practice\", \"warm-up\", \"cool-down\", \"goal\", \"scoring\",\n",
        "    \"penalty\", \"foul\", \"time-out\", \"half-time\", \"substitution\", \"tiebreaker\", \"extra time\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Nse4Awid2M93"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#data preparing"
      ],
      "metadata": {
        "id": "w8wQybfFFRcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame for AI-related data\n",
        "# 'ai_data' contains the text data related to AI concepts\n",
        "# 'class_name' column is set to 'ai' to label this dataset as belonging to AI category\n",
        "df_ai = pd.DataFrame({'text': ai_data, 'class_name': 'ai'})\n",
        "\n",
        "# Create a DataFrame for Sports-related data\n",
        "# 'Sports_data' contains the text data related to Sports concepts\n",
        "# 'class_name' column is set to 'sports' to label this dataset as belonging to Sports category\n",
        "df_sports = pd.DataFrame({'text': Sports_data, 'class_name': 'sports'})\n",
        "\n",
        "# Combine the two DataFrames (AI and Sports) into one DataFrame\n",
        "# The concat function combines the data vertically (stacking the rows)\n",
        "df = pd.concat([df_ai, df_sports])\n",
        "\n",
        "# You might have repeated this block, which is unnecessary as it's already done above.\n",
        "# Hence, the following lines are redundant and can be removed:\n",
        "# df_ai = pd.DataFrame({'clean': ai_data, 'class_name' : 'ai'})\n",
        "# df_sports = pd.DataFrame({'clean': Sports_data , 'class_name' : 'sports'})\n",
        "# df = pd.concat([df_ai, df_sports])\n",
        "\n",
        "# Add a new column 'class_id' to the DataFrame to assign numeric labels\n",
        "# If 'class_name' is 'ai', set 'class_id' to 1; otherwise, set it to 2 (for 'sports')\n",
        "df['class_id'] = np.where(df['class_name'] == \"ai\", 1, 2)\n",
        "\n",
        "\n",
        "print(df.shape)\n",
        "df=df.drop_duplicates()\n",
        "df=df.reset_index(drop=True)\n",
        "print(df.shape)\n",
        "# Add an 'id' column starting from 1\n",
        "df['id'] = range(1, len(df) + 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssnOkcUDCyUn",
        "outputId": "a84426a5-a7c4-4300-939d-b0a769787341"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(504, 3)\n",
            "(432, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['class_name'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "YQx_4L0-y2hw",
        "outputId": "8c962073-69e8-443d-f3c2-ba821e41ea4c"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "class_name\n",
              "sports    223\n",
              "ai        209\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class_name</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>sports</th>\n",
              "      <td>223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ai</th>\n",
              "      <td>209</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2vec:\n",
        "    \"\"\"\n",
        "    A class to handle text data using Word2Vec embeddings and classify the data with a K-Nearest Neighbors model.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    word2vec_model : gensim.models.Word2Vec\n",
        "        A pre-trained Word2Vec model that provides vector representations for words.\n",
        "\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame containing the text data and labels, with 'text' for input text and 'class_id' for labels.\n",
        "\n",
        "    X : np.ndarray\n",
        "        An array of averaged word vectors representing each text instance.\n",
        "\n",
        "    Y : np.ndarray\n",
        "        An array of labels corresponding to each text instance.\n",
        "\n",
        "    train_df : np.ndarray\n",
        "        Training set features obtained after splitting the data.\n",
        "\n",
        "    test_df : np.ndarray\n",
        "        Testing set features obtained after splitting the data.\n",
        "\n",
        "    train_labels : np.ndarray\n",
        "        Labels for the training set.\n",
        "\n",
        "    test_labels : np.ndarray\n",
        "        Labels for the testing set.\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    __init__(word2vec_model, df)\n",
        "        Initializes the Word2vec instance with the Word2Vec model and dataset, computes word vectors, and splits data.\n",
        "\n",
        "    get_average_word_vector(concept)\n",
        "        Computes the average word vector for the given text string (concept).\n",
        "\n",
        "    test_word2vec_model()\n",
        "        Trains a K-Nearest Neighbors model on the training data and evaluates its performance on the test data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, word2vec_model, df):\n",
        "        \"\"\"\n",
        "        Initializes the Word2vec instance with a pre-trained Word2Vec model and a DataFrame.\n",
        "        Computes average word vectors for each text entry in the DataFrame and splits data for training and testing.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        word2vec_model : gensim.models.Word2Vec\n",
        "            Pre-trained model used to compute word embeddings.\n",
        "\n",
        "        df : pandas.DataFrame\n",
        "            DataFrame containing text data in 'text' column and labels in 'class_id' column.\n",
        "        \"\"\"\n",
        "        self.word2vec_model = word2vec_model\n",
        "        self.df = df\n",
        "        # Compute average word vector for each text entry in the DataFrame\n",
        "        self.df['word2vec'] = self.df['text'].apply(self.get_average_word_vector)\n",
        "        self.X = np.array(self.df['word2vec'].tolist())\n",
        "        self.Y = np.array(self.df['class_id'])\n",
        "\n",
        "        # Split the data into training and testing sets\n",
        "        self.train_df, self.test_df, self.train_labels, self.test_labels = train_test_split(\n",
        "            self.X, self.Y, test_size=0.2, random_state=42, shuffle=True\n",
        "        )\n",
        "\n",
        "    def get_average_word_vector(self, concept):\n",
        "        \"\"\"\n",
        "        Computes the average word vector for a given text string (concept) by averaging the vectors of individual words.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        concept : str\n",
        "            The input text string for which the vector representation is calculated.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            A numpy array representing the averaged word vector for the input text string.\n",
        "        \"\"\"\n",
        "        concept_vector = np.zeros((300,), dtype=float)\n",
        "        valid_word_count = 0\n",
        "\n",
        "        # Calculate the sum of word vectors for each valid word in the input text\n",
        "        for word in word_tokenize(concept):\n",
        "            if word in self.word2vec_model:\n",
        "                concept_vector += self.word2vec_model[word]\n",
        "                valid_word_count += 1\n",
        "\n",
        "        # If no valid word vectors are found, return a zero vector; otherwise, return the average vector\n",
        "        return concept_vector if valid_word_count == 0 else concept_vector / valid_word_count\n",
        "\n",
        "    def test_word2vec_model(self):\n",
        "        \"\"\"\n",
        "        Trains a K-Nearest Neighbors (KNN) classifier using the training data and evaluates its accuracy and F1 score on the test set.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple\n",
        "            A tuple containing the accuracy and F1 score of the KNN classifier on the test data.\n",
        "        \"\"\"\n",
        "        knn = KNeighborsClassifier(n_neighbors=3)\n",
        "        knn.fit(self.train_df, self.train_labels)\n",
        "        y_pred = knn.predict(self.test_df)\n",
        "        accuracy = accuracy_score(self.test_labels, y_pred)\n",
        "        f1 = f1_score(self.test_labels, y_pred)\n",
        "        return accuracy, f1\n",
        "\n",
        "\n",
        "import time\n",
        "start_time = time.time()  # Start the timer\n",
        "obj = Word2vec(wv, df)\n",
        "# Test the Word2Vec model using the K-Nearest Neighbors classifier and display results\n",
        "accuracy, f1 = obj.test_word2vec_model()\n",
        "print(\"Evaluation Results for K-Nearest Neighbors Classifier:\")\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"F1 Score: {f1:.2f}%\")\n",
        "end_time = time.time()  # End the timer\n",
        "runtime = end_time - start_time  # Calculate the elapsed time\n",
        "print(f\"Pipeline ran time seconds :\" ,runtime)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WtWRsOTm5oP",
        "outputId": "813e8fc7-396a-4bfe-fdad-f70773201e3a"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results for K-Nearest Neighbors Classifier:\n",
            "Accuracy: 82.76%\n",
            "F1 Score: 0.86%\n",
            "Pipeline ran time seconds : 0.06171560287475586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the embeddings model for document indexing\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVrz5K8A1vQE",
        "outputId": "87c9a7b8-3924-460a-d145-ba4f40ecc01b"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SentenceTransformers:\n",
        "    \"\"\"\n",
        "    A class to manage sentence embeddings and similarity-based classification using a vector database.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    embeddings : object\n",
        "        The embedding model used to transform text into vector representations.\n",
        "    df : DataFrame\n",
        "        A DataFrame containing text data and associated labels.\n",
        "    train_df : DataFrame\n",
        "        Training split of the provided DataFrame.\n",
        "    test_df : DataFrame\n",
        "        Testing split of the provided DataFrame.\n",
        "    vector_store : object\n",
        "        A vector database instance used for similarity search.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    __init__(embeddings, df):\n",
        "        Initializes the SentenceTransformers class with an embeddings model and a DataFrame.\n",
        "\n",
        "    vector_db():\n",
        "        Creates and initializes a Chroma vector database client, clears the existing data, and adds new documents with metadata.\n",
        "\n",
        "    follow_steps():\n",
        "        Performs similarity search for each test document, prints the most similar documents, and identifies the most common class ID.\n",
        "\n",
        "    test():\n",
        "        Evaluates the model's accuracy and F1 score by predicting the most common class for each test document based on the top-3 similar documents.\n",
        "\n",
        "    pipline():\n",
        "        Runs the full pipeline of vector database setup, similarity search, and evaluation, returning the accuracy and F1 score.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embeddings, df):\n",
        "        \"\"\"\n",
        "        Initializes the SentenceTransformers class with an embedding model and DataFrame.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        embeddings : object\n",
        "            The embedding model used to generate vector representations of text.\n",
        "        df : DataFrame\n",
        "            A DataFrame with columns 'text' for document content and 'class_id' for classification labels.\n",
        "        \"\"\"\n",
        "        self.embeddings = embeddings\n",
        "        self.df = df\n",
        "        # Splits the data into training and testing sets.\n",
        "        self.train_df, self.test_df = train_test_split(\n",
        "            df, test_size=0.2, random_state=42, shuffle=True\n",
        "        )\n",
        "\n",
        "    def vector_db(self):\n",
        "        \"\"\"\n",
        "        Sets up the Chroma vector database client, removes any existing collection data,\n",
        "        creates a new collection, and adds training documents with an 'id' metadata field.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        vector_store : object\n",
        "            The initialized Chroma vector store with added documents for similarity search.\n",
        "        \"\"\"\n",
        "        # Initialize Chroma client and clear any existing collection.\n",
        "        client = chromadb.Client()\n",
        "        client.delete_collection(name=\"collection\")\n",
        "\n",
        "        # Create or retrieve the collection and add embeddings\n",
        "        collection = client.get_or_create_collection(name=\"collection\")\n",
        "        self.vector_store = Chroma(\n",
        "            client=client,\n",
        "            collection_name=\"collection\",\n",
        "            embedding_function=self.embeddings\n",
        "        )\n",
        "\n",
        "        # Prepare and add documents to the vector store\n",
        "        documents = [Document(page_content=t, metadata={\"id\": i-1}) for i, t in zip(self.train_df['id'][:], self.train_df['text'][:])]\n",
        "        self.vector_store.add_documents(documents)\n",
        "\n",
        "        return self.vector_store\n",
        "\n",
        "    def follow_steps(self):\n",
        "        \"\"\"\n",
        "        For each document in the test set, retrieves the top-3 most similar documents, prints similarity details,\n",
        "        and identifies the most frequent class among the similar documents, along with the true class.\n",
        "        \"\"\"\n",
        "        for idx, (text, true_class) in enumerate(zip(self.test_df['text'], self.test_df['class_id'])):\n",
        "            most_similar_docs = self.vector_store.similarity_search(text, k=3)\n",
        "            print(\"Query Text:\", text)\n",
        "            print(\"Most Similar Documents:\")\n",
        "            output_labels = []\n",
        "            for idx, sim_doc in enumerate(most_similar_docs):\n",
        "                class_id = self.df.loc[self.df['id'] == sim_doc.metadata['id'], 'class_id'].values[0]\n",
        "                print(f\"Rank {idx + 1}:\")\n",
        "                print(\"Similar Text:\", sim_doc.page_content)\n",
        "                print(\"Class ID:\", class_id)\n",
        "                print(\"-\" * 30)\n",
        "                output_labels.append(class_id)\n",
        "\n",
        "            item_counts = Counter(output_labels)\n",
        "            most_common_item = item_counts.most_common(1)[0]\n",
        "            print(\"Most Common Item:\", most_common_item[0])\n",
        "            print(\"True Class:\", true_class)\n",
        "\n",
        "    def test(self):\n",
        "        \"\"\"\n",
        "        Evaluates the model on the test set by predicting the most frequent class among the top-3 similar documents\n",
        "        for each test document. Calculates accuracy and F1 score.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        accuracy : float\n",
        "            The accuracy score for the model predictions on the test set.\n",
        "        f1 : float\n",
        "            The F1 score for the model predictions on the test set.\n",
        "        \"\"\"\n",
        "        predicted_out = []\n",
        "        true_labels = self.test_df['class_id'].tolist()\n",
        "        for text in self.test_df['text']:\n",
        "            most_similar_docs = self.vector_store.similarity_search(text, k=3)\n",
        "            output_labels = []\n",
        "            for sim_doc in most_similar_docs:\n",
        "                class_id = self.df.loc[self.df['id'] == sim_doc.metadata['id'], 'class_id'].values[0]\n",
        "                output_labels.append(class_id)\n",
        "\n",
        "            item_counts = Counter(output_labels)\n",
        "            most_common_item = item_counts.most_common(1)[0]\n",
        "            predicted_out.append(most_common_item[0])\n",
        "\n",
        "        return accuracy_score(true_labels, predicted_out), f1_score(true_labels, predicted_out)\n",
        "\n",
        "    def pipline(self):\n",
        "        \"\"\"\n",
        "        Executes the full pipeline: setting up the vector database, performing similarity search for each test document,\n",
        "        and evaluating the model's accuracy and F1 score.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        accuracy : float\n",
        "            The accuracy score after evaluating the model.\n",
        "        f1 : float\n",
        "            The F1 score after evaluating the model.\n",
        "        \"\"\"\n",
        "        self.vector_db()\n",
        "        self.follow_steps()\n",
        "        accuracy, f1 = self.test()\n",
        "        return accuracy, f1\n",
        "\n",
        "\n",
        "import time\n",
        "start_time = time.time()  # Start the timer\n",
        "\n",
        "sent_obj = SentenceTransformers(embeddings, df)\n",
        "accuracy, f1 = sent_obj.pipline()\n",
        "print(\"Evaluation Results for Sentence Transformers:\")\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"F1 Score: {f1:.2f}%\")\n",
        "\n",
        "end_time = time.time()  # End the timer\n",
        "runtime = end_time - start_time  # Calculate the elapsed time\n",
        "print(f\"Pipeline ran time seconds :\" ,runtime)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCs0XqJv1EmE",
        "outputId": "805e029d-d123-4874-faab-688683b08af1"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query Text: personal best\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: prediction\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: accuracy\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: self-attention\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 2\n",
            "Query Text: autoencoders\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: deep learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: generative adversarial networks\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: deep neural networks\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: scalability\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: performance\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: speed\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: performance metrics\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 1\n",
            "Query Text: speech recognition\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: ASR (automatic speech recognition)\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: deep learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: neural networks\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: powerlifting\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: weightlifting\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: strength training\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: strength\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: race\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: marathon\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: running\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: relay race\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: coordination\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: balance\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: strategy\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: skills\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: gradient boosting\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: gradient descent\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: natural gradient\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: vectorization\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: meta-learning\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: self-supervised learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: machine learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: few-shot learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: weighted loss\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: gradient descent\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: stochastic gradient descent\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: transfer learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: counterfactual analysis\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: inference\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: fairness in AI\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: counterattack\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: data pipeline\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: pipeline orchestration\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: data processing\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: data wrangling\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: hierarchical clustering\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: clustering\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: k-means clustering\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: spectral clustering\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: team captain\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: team\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: team sports\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: coach\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: backpropagation\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: gradient descent\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: neural networks\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: stochastic gradient descent\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: word2vec\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: word embeddings\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: text embeddings\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: subword tokenization\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: pole vault\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: jumping ability\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: climbing\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: long jump\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: neural Turing machines\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: recurrent neural networks\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: memory networks\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: neural networks\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: cognitive computing\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: quantum computing in AI\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: data processing\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: signal processing\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: teamwork\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: team\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: team sports\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: skills\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: huddle\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: grappling\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: fast break\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: cycling\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: cricket\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: baseball\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: bowling\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: umpire\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: warm-up\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: runner-up\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: workout\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: uniform\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: loss function\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: gradient descent\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: stochastic gradient descent\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: neural networks\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: stretches\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: workout\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: weightlifting\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: jogging\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: forward pass\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: passing\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: crosscourt\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: free throw\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: maximum likelihood estimation\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: Bayesian inference\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: Bayesian networks\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: machine learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: motor racing\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: competition\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: relay race\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: championship\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: player\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: athlete\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: team\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: defender\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: quarterback\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: football\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: superbowl\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: goalkeeper\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: Markov decision process\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: Markov chains\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: probabilistic programming\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: reinforcement learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: paragliding\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: jumping ability\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: speedwork\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: long jump\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: finals\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: playoff\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: championship\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: tournament\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: boosting\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: ensemble learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: vectorization\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: clustering\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: cool-down\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: time-out\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: snowmobiling\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: runner-up\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: recommender systems\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: information retrieval\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: probabilistic graphical models\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: machine learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: ultimate frisbee\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: golf\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: paddle\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: softball\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: support vector regression\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: support vector machines\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: vectorization\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: regression\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: shot put\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: golf\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: basketball\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: tennis\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: scoring\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: score\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: scoreboard\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: goal\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: field hockey\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: hockey\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: team sports\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: individual sports\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: striker\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: goalkeeper\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: defender\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: midfield\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: contextualized embeddings\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: text embeddings\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: word embeddings\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: representation learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: dynamic time warping\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: vectorization\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: clustering\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: spectral clustering\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: virtual assistants\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: virtual reality\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: cloud AI services\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: automation\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: data lake\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: data wrangling\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: data warehouse\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: big data\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: shooting\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: basketball\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: archery\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: defense\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: deep autoencoders\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: deep neural networks\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: deep learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: convolutional networks\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: opponent\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: match\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: tournament\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: competition\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: freestyle\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: swimming\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: long jump\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: speed skating\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: detection\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: classification\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: computer vision\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: signal processing\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: interval training\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: training set\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: training\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: training\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 2\n",
            "Query Text: kickboxing\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: kick\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: goal kick\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: corner kick\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: algorithm\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: clustering\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: data mining\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: machine learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: Internet of Things (IoT)\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: smart cities\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: big data\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: edge computing\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: ring\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: championship ringathletics\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: boxing\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: wrestler\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: substitute\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: substitution\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: augmentation\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: sentence transformers\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 2\n",
            "Query Text: model evaluation\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: model interpretability\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: language models\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: regression analysis\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: trampoline\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: bounce\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: skateboarding\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: long jump\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: GPT\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: tournament\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: medal\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: golf\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 1\n",
            "Query Text: ice skating\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: figure skating\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: speed skating\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: skateboarding\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: injury\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: injury prevention\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: recovery\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: offense\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: feature selection\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: feature extraction\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: feature engineering\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: machine learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: feature vectors\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: vectorization\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: feature extraction\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: support vector machines\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: skiing\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: snowboarding\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: snowmobiling\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: mountaineering\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: predictive modeling\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: prediction\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: ensemble models\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: machine learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: contextual embeddings\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: text embeddings\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: word embeddings\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: representation learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: privacy-preserving machine learning\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: machine learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: self-supervised learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: adversarial networks\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: causal inference\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: inference\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: Bayesian inference\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: Bayesian networks\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: big data analytics\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: big data\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: analytics\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: data mining\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: bounce pass\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: bounce\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: passing\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: corner kick\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: strategies\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: strategy\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: tactics\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: technique\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: random forest\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: decision trees\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: classification\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: machine learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: stacked models\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: ensemble models\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: ensemble learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: language models\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: recovery time\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: recovery\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: rebound\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: endurance\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: defensive\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: defense\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: defender\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: offense\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: natural language processing\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: sentiment analysis\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: word embeddings\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: information retrieval\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: data augmentation\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: augmentation\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: data wrangling\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: data processing\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: variational autoencoders\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: generative adversarial networks\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: deep learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: deep neural networks\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: convergence\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: prediction\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: gradient descent\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: time series forecasting\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: bagging\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: paddle\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: snowmobiling\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: golf\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 1\n",
            "Query Text: javelin throw\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: free throw\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: gymnastics\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: softball\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 2\n",
            "Query Text: neural network architecture\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: neural networks\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: artificial neural networks\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: deep neural networks\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: linear discriminant analysis\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: principal component analysis\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: dimensionality reduction\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: spectral clustering\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: optimization\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: hyperparameter optimization\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: gradient descent\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: parameter tuning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: genetic algorithms\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: machine learning\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: artificial neural networks\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: data mining\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Most Common Item: 1\n",
            "True Class: 1\n",
            "Query Text: weights\n",
            "Most Similar Documents:\n",
            "Rank 1:\n",
            "Similar Text: weightlifting\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Rank 2:\n",
            "Similar Text: sample weighting\n",
            "Class ID: 1\n",
            "------------------------------\n",
            "Rank 3:\n",
            "Similar Text: strength training\n",
            "Class ID: 2\n",
            "------------------------------\n",
            "Most Common Item: 2\n",
            "True Class: 1\n",
            "Evaluation Results for Sentence Transformers:\n",
            "Accuracy: 91.95%\n",
            "F1 Score: 0.93%\n",
            "Pipeline ran time seconds : 6.060160398483276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "comments on results\n",
        "# **sentence transformers capture semantic meaning best than word2vec but run time would be long compared with word2vec i used vector db to imporve run  time  , word2vec has limited vocabulary to depend on training data**"
      ],
      "metadata": {
        "id": "L9PNWN58ynyg"
      }
    }
  ]
}